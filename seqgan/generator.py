import torch
import torch.nn as nn
import torch.nn.functional as F


class Generator(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, use_cuda):
        super(Generator, self).__init__()
        self.hidden_dim = hidden_dim
        self.use_cuda = use_cuda
        self.embed = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        self.log_softmax = nn.LogSoftmax(dim=1)
        self.init_params()

    def forward(self, x):
        """
        Embeds input and applies LSTM on the input sequence.

        Args:
            x: (batch_size, seq_len), sequence of tokens generated by generator

        Returns:
            Generated output of size (batch_size * seq_len, vocab_size)
        """
        self.lstm.flatten_parameters()
        h0, c0 = self.init_hidden(x.size(0))
        emb = self.embed(x)  # batch_size * seq_len * emb_dim
        out, _ = self.lstm(emb, (h0, c0))  # out: batch_size * seq_len * hidden_dim
        out = self.log_softmax(
            self.fc(out.contiguous().view(-1, self.hidden_dim))
        )  # (batch_size*seq_len) * vocab_size
        return out

    def step(self, x, h, c):
        """
        Embed the input and apply LSTM one token at a time (seq_len = 1).

        Args: x, h, c
            x: (batch_size, 1), sequence of tokens generated by generator
            h: (1, batch_size, hidden_dim), lstm hidden state
            c: (1, batch_size, hidden_dim), lstm cell state

        Returns:
            out: (batch_size, vocab_size), lstm output prediction
            h: (1, batch_size, hidden_dim), lstm hidden state
            c: (1, batch_size, hidden_dim), lstm cell state
        """
        self.lstm.flatten_parameters()
        emb = self.embed(x)  # batch_size * 1 * emb_dim
        out, (h, c) = self.lstm(emb, (h, c))  # out: batch_size * 1 * hidden_dim
        out = self.log_softmax(
            self.fc(out.contiguous().view(-1, self.hidden_dim))
        )  # batch_size * vocab_size
        return out, h, c

    def init_hidden(self, batch_size):
        h = torch.zeros(1, batch_size, self.hidden_dim)
        c = torch.zeros(1, batch_size, self.hidden_dim)
        if self.use_cuda:
            h, c = h.cuda(), c.cuda()
        return h, c

    def init_params(self):
        for param in self.parameters():
            param.data.uniform_(-0.05, 0.05)

    def sample(self, batch_size, seq_len, x=None):
        """
        Samples the network and returns a batch of samples of length seq_len.

        Returns:
            Tensor (batch_size * seq_len)
        """
        samples = []
        if x is None:
            h, c = self.init_hidden(batch_size)
            x = torch.zeros(batch_size, 1, dtype=torch.int64)
            if self.use_cuda:
                x = x.cuda()
            for _ in range(seq_len):
                out, h, c = self.step(x, h, c)
                prob = torch.exp(out)
                x = torch.multinomial(prob, 1)
                samples.append(x)
        else:
            h, c = self.init_hidden(x.size(0))
            given_len = x.size(1)
            lis = x.chunk(x.size(1), dim=1)
            for i in range(given_len):
                out, h, c = self.step(lis[i], h, c)
                samples.append(lis[i])
            prob = torch.exp(out)
            x = torch.multinomial(prob, 1)
            for _ in range(given_len, seq_len):
                samples.append(x)
                out, h, c = self.step(x, h, c)
                prob = torch.exp(out)
                x = torch.multinomial(prob, 1)
        out = torch.cat(samples, dim=1)  # along the batch_size dimension
        return out


# class Generator(nn.Module):
#     def __init__(
#         self,
#         vocab_size,
#         embedding_dim,
#         hidden_dim,
#         rnn_type="lstm",
#         num_layers=1,
#         dropout=0.0,
#     ):
#         assert rnn_type in ["lstm", "gru"]

#         super().__init__()

#         self.vocab_size = vocab_size
#         self.embedding_dim = embedding_dim
#         self.hidden_dim = hidden_dim
#         self.rnn_type = rnn_type
#         self.num_layers = num_layers
#         self.dropout = dropout

#         self.embeddings = nn.Embedding(vocab_size, embedding_dim)
#         if rnn_type == "lstm":
#             self.rnn = nn.LSTM(
#                 embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True
#             )
#         else:
#             self.rnn = nn.GRU(
#                 embedding_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True
#             )
#         self.fc = nn.Linear(hidden_dim, vocab_size)

#         self.hidden_state = None

#     def forward(self, x, reset_hidden_state=True):
#         x = self.embeddings(x)
#         if reset_hidden_state:
#             x, _ = self.rnn(x)
#         else:
#             x, self.hidden_state = self.rnn(x, self.hidden_state)
#         x = self.fc(x)
#         return x

#     def generate_sequence(self, seq_start=None, max_length=1024, **kwargs):
#         if not seq_start:
#             seq_start = [BOS_TOKEN]
#         seq = seq_start.copy()
#         with torch.no_grad():
#             next_token = self._generate_next_token(
#                 next_input=torch.LongTensor(seq).to(device), reset_hidden=True, **kwargs
#             )
#             while len(seq) <= max_length:
#                 next_token = self._generate_next_token(
#                     next_input=torch.LongTensor([next_token]).to(device),
#                     reset_hidden=False,
#                     **kwargs
#                 )
#                 seq.append(next_token)
#         return seq

#     def _generate_next_token(
#         self, next_input, reset_hidden=False, temp=1.0, topk=5, argmax=False
#     ):
#         # The model expects a batch input, so we add a fake batch dimension.
#         model_input = next_input.unsqueeze(0)
#         # Then, we need to remove the fake batch dimension from the output.
#         model_output = self(model_input, reset_hidden).squeeze(0)
#         next_token_probs = F.softmax(model_output[-1] / temp, dim=0)
#         if argmax:
#             next_token = torch.argmax(next_token_probs)
#         else:
#             top_tokens = torch.topk(next_token_probs, topk)
#             top_indices = top_tokens.indices
#             top_probs = top_tokens.values
#             top_probs /= torch.sum(top_probs)
#             next_token = np.random.choice(
#                 top_indices.cpu().numpy(), p=top_probs.cpu().numpy()
#             )
#         return next_token.item()